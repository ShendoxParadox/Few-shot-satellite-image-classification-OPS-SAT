{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdcd3ca",
   "metadata": {},
   "source": [
    "# OPS-SAT case starter-kit notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad0860",
   "metadata": {},
   "source": [
    "ESA's [Kelvins](https://kelvins.esa.int) competition \"[the OPS-SAT case](https://kelvins.esa.int/opssat/home/)\" is a novel data-centric challenge that asks you to work with the raw data of a satellite and very few provided labels to find the best parameters for a given machine learning model. Compared to previous competitions on Kelvins (like the [Pose Estimation](https://kelvins.esa.int/pose-estimation-2021/) or the [Proba-V Super-resolution challenge](https://kelvins.esa.int/proba-v-super-resolution/)) where the test-set is provided and the infered results are submitted, for the OPS-SAT case, we will run inference on the Kelvins server directly! This notebooks contains examples on how you can load your data and train an **EfficientNetLite0** model by only using the 80-labeled images provided. Therefore, the directory `images`, containing unlabeld patches and included in the training dataset is not used for this notebook. However, competitors are encouraged to use these patches to improve the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403eeb5a",
   "metadata": {},
   "source": [
    "# 1. Module imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d88f44",
   "metadata": {},
   "source": [
    "If you do not have a GPU, uncomment and run the next commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f3f6cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:31.525217Z",
     "start_time": "2023-04-09T20:42:31.521341Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.16 (main, Mar  8 2023, 14:00:05) \\n[GCC 11.2.0]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7c8f9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:31.887841Z",
     "start_time": "2023-04-09T20:42:31.711974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/zsh: /home/ramez/miniconda3/envs/thesis/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb8fddce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:32.267814Z",
     "start_time": "2023-04-09T20:42:32.094731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/zsh: /home/ramez/miniconda3/envs/thesis/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "/home/ramez/miniconda3/envs/thesis/bin/python\n"
     ]
    }
   ],
   "source": [
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3aecaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:33.914315Z",
     "start_time": "2023-04-09T20:42:33.911985Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1fd19-d4ef-4a54-877e-9d06a9049737",
   "metadata": {},
   "source": [
    "#### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1e02f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.255529Z",
     "start_time": "2023-04-09T20:42:34.328075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/zsh: /home/ramez/miniconda3/envs/thesis/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "234eba6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.260638Z",
     "start_time": "2023-04-09T20:42:36.257966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea21b9b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.327202Z",
     "start_time": "2023-04-09T20:42:36.262029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f73af189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.336400Z",
     "start_time": "2023-04-09T20:42:36.333721Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80fbbb42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.345491Z",
     "start_time": "2023-04-09T20:42:36.343286Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Local EfficientNetLite (Customized by the Competition)\n",
    "from efficientnet_lite import EfficientNetLiteB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6face98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.349008Z",
     "start_time": "2023-04-09T20:42:36.346850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb004876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.353039Z",
     "start_time": "2023-04-09T20:42:36.350804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22d591c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:36.719018Z",
     "start_time": "2023-04-09T20:42:36.716802Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eda62",
   "metadata": {},
   "source": [
    "# 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec32f1b",
   "metadata": {},
   "source": [
    "You can use this function to load your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f270337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:37.464808Z",
     "start_time": "2023-04-09T20:42:37.461420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_images_from_path(dataset_path):\n",
    "    \"\"\" Get images from path and normalize them applying channel-level normalization. \"\"\"\n",
    "\n",
    "    # loading all images in one large batch\n",
    "    tf_eval_data = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], shuffle=False, \n",
    "                                                               batch_size=100000)\n",
    "\n",
    "    # extract images and targets\n",
    "    for tf_eval_images, tf_eval_targets in tf_eval_data:\n",
    "        break\n",
    "\n",
    "    return tf.convert_to_tensor(tf_eval_images), tf_eval_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625d53c",
   "metadata": {},
   "source": [
    "# 3. Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c35913",
   "metadata": {},
   "source": [
    "The network architecture used for OPS-SAT is **EfficientNetLite0**. We would like to thank Sebastian for making a Keras implementation of EfficientNetLite publicly available under the Apache 2.0 License: https://github.com/sebastian-sz/efficientnet-lite-keras. Our Version of this code has been modified to better fit our purposes. For example, we removed the ReLU \"stem_activation\" to better match a related efficientnet pytorch implementation. In any way, **you have to use the model architecture that we provide in our [starter-kit](https://gitlab.com/EuropeanSpaceAgency/the_opssat_case_starter_kit).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5fe37a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:38.494116Z",
     "start_time": "2023-04-09T20:42:38.491962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = (200, 200, 3)   # input_shape is (height, width, number of channels) for images\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867a878-90ed-426b-8db3-6726c8cd8a7a",
   "metadata": {},
   "source": [
    "## Load The Model Without any Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d718f5fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:39.783353Z",
     "start_time": "2023-04-09T20:42:39.220320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = EfficientNetLiteB0(classes=num_classes, weights=None, input_shape=input_shape, classifier_activation=None)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f71ef3-0bb2-4c17-9f08-9c2a0efae2f3",
   "metadata": {},
   "source": [
    "## Load The Model With ImageNet Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07e71688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:41.669094Z",
     "start_time": "2023-04-09T20:42:40.995565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1a_ same\n",
      "block2a_ ((1, 1), (1, 1))\n",
      "block2a_ valid\n",
      "block2b_ same\n",
      "block3a_ ((2, 2), (2, 2))\n",
      "block3a_ valid\n",
      "block3b_ same\n",
      "block4a_ ((1, 1), (1, 1))\n",
      "block4a_ valid\n",
      "block4b_ same\n",
      "block4c_ same\n",
      "block5a_ same\n",
      "block5b_ same\n",
      "block5c_ same\n",
      "block6a_ ((2, 2), (2, 2))\n",
      "block6a_ valid\n",
      "block6b_ same\n",
      "block6c_ same\n",
      "block6d_ same\n",
      "block7a_ same\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetLiteB0(classes=num_classes, weights='imagenet', input_shape=input_shape, classifier_activation=None, include_top = False)\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=model.input, outputs=output_layer)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cec9e1",
   "metadata": {},
   "source": [
    "# 4. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3939168c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:42.576907Z",
     "start_time": "2023-04-09T20:42:42.574308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path_train=\"/home/ramez/Politechnika_Slaska_MSc/Thesis/Competition/Data/images_copy_processed/\"\n",
    "dataset_path_train_val = \"/home/ramez/Politechnika_Slaska_MSc/Thesis/Competition/Data/ops_sat_train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e57cb03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:43.086890Z",
     "start_time": "2023-04-09T20:42:43.084818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path_test = \"/home/ramez/Politechnika_Slaska_MSc/Thesis/Competition/Data/ops_sat_test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7277b8",
   "metadata": {},
   "source": [
    "In this notebook, classical supervised learning is used. Therefore, remember to remove the subdirectory `images` containing unlabeled patches before loading the dataset to perform training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "535d9f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:44.029535Z",
     "start_time": "2023-04-09T20:42:43.904140Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 227 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset\n",
    "x_train_val, y_train_val = get_images_from_path(dataset_path_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "176ad7e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:44.501742Z",
     "start_time": "2023-04-09T20:42:44.458845Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset\n",
    "x_test, y_test = get_images_from_path(dataset_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed865db",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99bf6b",
   "metadata": {},
   "source": [
    "We provide now an example on how you can train your model by using standard supervised learning. Training loss (`SparseCategoricalCrossentropy`) and `Accuracy` are shown for simplicity and for an easier interpretation of the training outcome, despite your submission will be evaluated by using the metric **1 - Cohen's kappa** [metric](https://en.wikipedia.org/wiki/Cohen's_kappa). For more information on scoring, please refer to [Scoring](https://kelvins.esa.int/opssat/scoring/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f87e9",
   "metadata": {},
   "source": [
    "With this model and the dataset provided, please do your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86fe85-998e-4704-bb76-18af67274166",
   "metadata": {},
   "source": [
    "### Compile The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "339bc248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:45.707392Z",
     "start_time": "2023-04-09T20:42:45.698280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c29753-9cbf-4149-a938-ceec8435dce0",
   "metadata": {},
   "source": [
    "### Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0fed412c-c892-4aa3-ad9a-fe9f4fce908f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc53411-de66-41a5-bdfe-47e6b2b94462",
   "metadata": {},
   "source": [
    "### Without K fold Cross Validation but with TensorBoard (With Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ae962a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:49.259862Z",
     "start_time": "2023-04-09T20:42:49.257217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard_callback = TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24568775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:43:38.960294Z",
     "start_time": "2023-04-09T20:42:50.924320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# history = model.fit(x_train_val, y_train_val, validation_data=(x_test, y_test), epochs= 100, verbose=1, batch_size=8, \n",
    "#                         callbacks=[early_stopping, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faa7c10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:45:36.527054Z",
     "start_time": "2023-04-09T20:44:17.923419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9899b0-587d-4c9a-b9b5-ec61295e2e48",
   "metadata": {},
   "source": [
    "### With K fold Cross Validation but without TensorBoard (With Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12efbbf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:42:47.226837Z",
     "start_time": "2023-04-09T20:42:47.224186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a930432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:43:57.248568Z",
     "start_time": "2023-04-09T20:43:57.245137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 3s 120ms/step - loss: 0.2388 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 3s 118ms/step - loss: 0.1480 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.4565 - val_sparse_categorical_accuracy: 0.9348\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.1172 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.0894 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9779 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 3s 118ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0263 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.0183 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0465 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 3s 117ms/step - loss: 0.0356 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0099 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 3s 125ms/step - loss: 0.0236 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0092 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 3s 121ms/step - loss: 0.0138 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0056 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 3s 119ms/step - loss: 0.0275 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0013 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0010 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0019 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 3s 118ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0022 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.0661 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.0044 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 3s 120ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0121 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0104 - val_sparse_categorical_accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0104 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 3s 118ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.0722 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 3s 125ms/step - loss: 0.1350 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.0248 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.0046 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 3s 119ms/step - loss: 0.1032 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.0097 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0476 - sparse_categorical_accuracy: 0.9779 - val_loss: 0.1216 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 3s 125ms/step - loss: 0.1144 - sparse_categorical_accuracy: 0.9613 - val_loss: 0.1559 - val_sparse_categorical_accuracy: 0.9565\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 3s 119ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.0868 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.1019 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.0824 - val_sparse_categorical_accuracy: 0.9565\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0824 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 3s 127ms/step - loss: 0.1459 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.5276 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 3s 117ms/step - loss: 0.1726 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.5598 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 3s 125ms/step - loss: 0.2309 - sparse_categorical_accuracy: 0.9121 - val_loss: 0.9379 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 3s 125ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9505 - val_loss: 0.4331 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.9615 - val_loss: 1.8992 - val_sparse_categorical_accuracy: 0.8222\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.1028 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.4999 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 3s 127ms/step - loss: 0.0697 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.3861 - val_sparse_categorical_accuracy: 0.8889\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 3s 128ms/step - loss: 0.0689 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.4537 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 3s 120ms/step - loss: 0.0433 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.8315 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 3s 127ms/step - loss: 0.0588 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.0409 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0088 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0900 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.0059 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0386 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0078 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 3s 130ms/step - loss: 0.0118 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0050 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 3s 121ms/step - loss: 0.0699 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.0127 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0181 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0150 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 3s 129ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0169 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.0653 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0176 - val_sparse_categorical_accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0176 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.0769 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0051 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 3s 131ms/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0602 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 3s 121ms/step - loss: 0.1030 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.0163 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 3s 129ms/step - loss: 0.2209 - sparse_categorical_accuracy: 0.9505 - val_loss: 0.0547 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 3s 128ms/step - loss: 0.0934 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.1283 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.1996 - val_sparse_categorical_accuracy: 0.9111\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1996 - sparse_categorical_accuracy: 0.9111\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 3s 131ms/step - loss: 0.1150 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.0967 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 3s 123ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.0035 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 3s 127ms/step - loss: 0.0711 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0089 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.0540 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0049 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 3s 128ms/step - loss: 0.0628 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.0332 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 3s 131ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.4321 - val_sparse_categorical_accuracy: 0.8444\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.1585 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.0503 - val_sparse_categorical_accuracy: 0.9778\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model using K-fold cross-validation\n",
    "scores = []\n",
    "for train_idx, val_idx in kf.split(x_train_val):\n",
    "    X_train = tf.gather(x_train_val, train_idx)\n",
    "    y_train = tf.gather(y_train_val, train_idx)\n",
    "    X_val = tf.gather(x_train_val, val_idx)\n",
    "    y_val = tf.gather(y_train_val, val_idx)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs= 100, verbose=1, batch_size=8, \n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    score = model.evaluate(X_val, y_val)\n",
    "    scores.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c3ae736d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T20:44:00.262059Z",
     "start_time": "2023-04-09T20:44:00.259248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy mean: 0.97 (std 0.03)\n"
     ]
    }
   ],
   "source": [
    "# Print the mean validation accuracy\n",
    "print('Validation accuracy mean: {:.2f} (std {:.2f})'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013aa747-2788-4c43-a743-c29c8c378de4",
   "metadata": {},
   "source": [
    "Calculating the **1 - Cohen's kappa** score of the trained model on the trained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb08a64b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T20:10:20.518612Z",
     "start_time": "2023-04-04T20:10:19.125881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6571428571428571\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros(len(y_test), dtype=np.int8)\n",
    "# inference loop\n",
    "for e, (image, target) in enumerate(zip(x_test, y_test)):\n",
    "    image = np.expand_dims(np.array(image), axis=0)\n",
    "    output = model.predict(image)\n",
    "    predictions[e] = np.squeeze(output).argmax()\n",
    "#Keras model score\n",
    "score_keras = 1 - cohen_kappa_score(y_test.numpy(), predictions)\n",
    "print(\"Score:\",score_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f66b63",
   "metadata": {},
   "source": [
    "# 6. Saving and loading trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e885b",
   "metadata": {},
   "source": [
    "The trained model can be now saved by using HDF5-format that is the only accepted for submission. The name `test.h5` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6c73b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-30T19:46:06.491070Z",
     "start_time": "2022-10-30T19:46:06.274124Z"
    }
   },
   "outputs": [],
   "source": [
    "#Saving model\n",
    "# model.save_weights('test.h5')\n",
    "model.save_weights('/home/ramez/Politechnika_Slaska_MSc/Thesis/Competition/submission/submit_19.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97641ba7",
   "metadata": {},
   "source": [
    "The trained model can be also loaded for further testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0adb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-30T19:40:44.823406Z",
     "start_time": "2022-10-30T19:40:43.780920Z"
    }
   },
   "outputs": [],
   "source": [
    "model = EfficientNetLiteB0(classes=num_classes, weights=None, input_shape=input_shape, classifier_activation=None)\n",
    "# model = EfficientNetLiteB0(include_top=False, classes=num_classes, weights='imagenet', input_shape=input_shape, classifier_activation=None)\n",
    "model.load_weights('/home/ramez/Politechnika_Slaska_MSc/Thesis/Competition/submission/model_patterns_20epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d26b2b",
   "metadata": {},
   "source": [
    "The model will be now compiled and tested again. You should get the same score as before saving and loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0e495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-08T14:01:38.000493Z",
     "start_time": "2022-10-08T14:01:36.169912Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model shall be compiled before testing.\n",
    "model.compile()\n",
    "\n",
    "#Creating empty predictions\n",
    "predictions = np.zeros(len(y_train), dtype=np.int8)\n",
    "\n",
    "# inference loop\n",
    "for e, (image, target) in enumerate(zip(x_train, y_train)):\n",
    "    image = np.expand_dims(np.array(image), axis=0)\n",
    "    output = model.predict(image)\n",
    "    predictions[e] = np.squeeze(output).argmax()\n",
    "\n",
    "#Keras model score\n",
    "score_keras = 1 - cohen_kappa_score(y_train.numpy(), predictions)\n",
    "print(\"Score:\",score_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82416ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eedf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786276ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b71e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-12T18:20:06.668315Z",
     "start_time": "2022-10-12T18:20:06.652610Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd4279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
